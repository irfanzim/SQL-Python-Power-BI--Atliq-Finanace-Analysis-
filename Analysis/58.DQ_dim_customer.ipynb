{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e4e1e0-11ab-4dad-99a8-74ade951c37d",
   "metadata": {},
   "source": [
    "### Data Quality Check for dim_customer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6e05e-04dd-44cf-8d25-1454efa66a60",
   "metadata": {},
   "source": [
    "- This notebook performs structural, integrity, and consistency checks on dim_customer before it is consumed in Power BI reporting.\n",
    "- Power BI dataset will connects directly to gdb041.dim_customer.\n",
    "- Each row of this dataset represents details for a specific customer_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c976b0f-f4b8-4c95-8db7-64f8e15d5456",
   "metadata": {},
   "source": [
    "### Database configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48576115-31e2-49b4-8737-92e68cd3a4b4",
   "metadata": {},
   "source": [
    "This notebook reads database credentials from environment variables.\n",
    "Credentials are **not stored in the notebook or repository**.\n",
    "\n",
    "Required variables:\n",
    "\n",
    "- DB_USER\n",
    "- DB_PASSWORD\n",
    "- DB_HOST\n",
    "- DB_NAME\n",
    "\n",
    "For local development:\n",
    "\n",
    "- Create a `.env` file (see `.env.example`)\n",
    "- Load variables using `python-dotenv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e5558-995f-49bd-be6e-2592a70335ee",
   "metadata": {},
   "source": [
    "### 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa4e783-4ec3-4afe-a09e-f8b6b5d793d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for data manipulation and numerical operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import sql engine\n",
    "from sqlalchemy import create_engine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef6d81-01e8-4a85-8963-9baeb92cec4b",
   "metadata": {},
   "source": [
    "### 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa15a9b-2cfa-43e3-ad56-106a9dc3b512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_code</th>\n",
       "      <th>customer</th>\n",
       "      <th>market</th>\n",
       "      <th>platform</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90002012</td>\n",
       "      <td>Electricalsocity</td>\n",
       "      <td>India</td>\n",
       "      <td>Brick &amp; Mortar</td>\n",
       "      <td>Retailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90002013</td>\n",
       "      <td>Electricalslytical</td>\n",
       "      <td>India</td>\n",
       "      <td>Brick &amp; Mortar</td>\n",
       "      <td>Retailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90002010</td>\n",
       "      <td>Ebay</td>\n",
       "      <td>India</td>\n",
       "      <td>E-Commerce</td>\n",
       "      <td>Retailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90002011</td>\n",
       "      <td>Atliq Exclusive</td>\n",
       "      <td>India</td>\n",
       "      <td>Brick &amp; Mortar</td>\n",
       "      <td>Retailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90002014</td>\n",
       "      <td>Expression</td>\n",
       "      <td>India</td>\n",
       "      <td>Brick &amp; Mortar</td>\n",
       "      <td>Retailer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_code            customer market        platform   channel\n",
       "0      90002012    Electricalsocity  India  Brick & Mortar  Retailer\n",
       "1      90002013  Electricalslytical  India  Brick & Mortar  Retailer\n",
       "2      90002010                Ebay  India      E-Commerce  Retailer\n",
       "3      90002011     Atliq Exclusive  India  Brick & Mortar  Retailer\n",
       "4      90002014          Expression  India  Brick & Mortar  Retailer"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Required environment variables\n",
    "REQUIRED_ENV_VARS = [\"DB_USER\", \"DB_PASSWORD\", \"DB_HOST\", \"DB_NAME\"]\n",
    "\n",
    "# Validate environment variables (Fail Fast)\n",
    "missing_vars = [var for var in REQUIRED_ENV_VARS if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    sys.exit(\n",
    "        f\"\"\"\n",
    "        ❌ Missing required environment variables: {', '.join(missing_vars)}\n",
    "\n",
    "        Please set them in your .env file or system environment before running the script.\n",
    "        Example:\n",
    "            DB_USER=your_username\n",
    "            DB_PASSWORD=your_password\n",
    "            DB_HOST=localhost\n",
    "            DB_NAME=your_database\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# Create database engine\n",
    "engine = create_engine(\n",
    "    f\"mysql+mysqlconnector://{os.getenv('DB_USER')}:\"\n",
    "    f\"{os.getenv('DB_PASSWORD')}@\"\n",
    "    f\"{os.getenv('DB_HOST')}/\"\n",
    "    f\"{os.getenv('DB_NAME')}\"\n",
    ")\n",
    "\n",
    "# Define query\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    customer_code,\n",
    "    customer,\n",
    "    market,\n",
    "    platform,\n",
    "    channel\n",
    "FROM gdb041.dim_customer\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_customer = pd.read_sql_query(query, engine)\n",
    "    print(\"✅ Data loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"❌ Failed to load data from dim_customer: {e}\")\n",
    "\n",
    "# Preview\n",
    "df_customer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0604f-fa3f-418c-871e-286664b39ee5",
   "metadata": {},
   "source": [
    "### 3. Initial Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ee894c7-4241-4337-bd21-c4789b3526cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *** initial report ***\n",
      "----------------------------------------\n",
      "*** Structure:\n",
      "- Total Rows: 209\n",
      "- Total Columns: 5\n",
      "- Column Names: ['customer_code', 'customer', 'market', 'platform', 'channel']\n",
      "\n",
      " *** Data Types:\n",
      "  customer_code: object\n",
      "  customer: object\n",
      "  market: object\n",
      "  platform: object\n",
      "  channel: object\n",
      "\n",
      " *** Mixed Data Types:\n",
      "  No mixed data types found\n",
      "\n",
      "*** Distinct Values per Column:\n",
      "  customer_code: 209\n",
      "  customer: 74\n",
      "  market: 27\n",
      "  platform: 2\n",
      "  channel: 3\n",
      "\n",
      "*** Null Values and Percentages:\n",
      "  No null values found\n",
      "\n",
      "\n",
      "*** Duplicates: 0\n",
      "*** Negative or Zero Values:\n",
      "  No negative or zero values found\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def initial_report(df):\n",
    "    print(\" *** initial report ***\\n\" + \"-\"*40)\n",
    "\n",
    "    print(f\"*** Structure:\\n- Total Rows: {df.shape[0]}\\n- Total Columns: {df.shape[1]}\")\n",
    "    print(f\"- Column Names: {list(df.columns)}\\n\")\n",
    "\n",
    "    \n",
    "    print(\" *** Data Types:\")\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        print(f\"  {col}: {dtype}\")\n",
    "    print()\n",
    "\n",
    "    print(\" *** Mixed Data Types:\")\n",
    "    has_mixed_types = False\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            type_counts = df[col].apply(type).value_counts()\n",
    "            if len(type_counts) > 1:\n",
    "                has_mixed_types = True\n",
    "                print(f\"  {col}:\")\n",
    "                for t, count in type_counts.items():\n",
    "                    print(f\"    - {t.__name__}: {count}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {col}: Error checking types - {e}\")\n",
    "\n",
    "    if not has_mixed_types:\n",
    "        print(\"  No mixed data types found\")\n",
    "    print()\n",
    "\n",
    "    print(\"*** Distinct Values per Column:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"  {col}: {df[col].nunique()}\")\n",
    "    print()\n",
    "\n",
    "    print(\"*** Null Values and Percentages:\")\n",
    "    has_null_value=False\n",
    "    nulls = df.isnull().sum()\n",
    "    for col in df.columns:\n",
    "        pct_missing = np.mean(df[col].isnull())\n",
    "        if nulls[col] > 0: # Only print if there are missing values\n",
    "            has_null_value=True\n",
    "            print(f\"  {col}: Missing Values: {nulls[col]}, Pct: {round(pct_missing * 100, 3)}%\")\n",
    "    if not has_null_value:\n",
    "        print(\"  No null values found\")\n",
    "    print()\n",
    "\n",
    "    \n",
    "    print(f\"\\n*** Duplicates: {df.duplicated().sum()}\")\n",
    "\n",
    "    print(\"*** Negative or Zero Values:\")\n",
    "    has_issues = False\n",
    "    for col in df.select_dtypes(include='number').columns:\n",
    "        zero_count = (df[col] == 0).sum()\n",
    "        negative_count = (df[col] < 0).sum()\n",
    "    \n",
    "        if zero_count > 0 or negative_count > 0:\n",
    "            has_issues = True\n",
    "            print(f\"  {col}:\")\n",
    "            if zero_count > 0:\n",
    "                print(f\"    - Zero values: {zero_count}\")\n",
    "            if negative_count > 0:\n",
    "                print(f\"    - Negative values: {negative_count}\")\n",
    "\n",
    "    if not has_issues:\n",
    "        print(\"  No negative or zero values found\")\n",
    "    print()\n",
    "\n",
    "initial_report(df_customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e1827a-99d4-48b5-964c-8d8eaef2cbec",
   "metadata": {},
   "source": [
    "### 4. Check what customers are available in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1647100e-2d32-4607-b20c-28b13b1f53e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer\n",
       "Acclaimed Stores        2\n",
       "All-Out                 1\n",
       "Amazon                 29\n",
       "Argos (Sainsbury's)     3\n",
       "Atlas Stores            2\n",
       "                       ..\n",
       "Unity Stores            1\n",
       "Vijay Sales             1\n",
       "Viveks                  1\n",
       "Zone                    2\n",
       "walmart                 2\n",
       "Name: count, Length: 74, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display value counts for each column to validate uniqueness and detect potential data quality issues.\n",
    "df_customer[\"customer\"].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e688591-ba44-46aa-a690-c737816e0573",
   "metadata": {},
   "source": [
    "### 4.1 Identify issues in customer data (whitespace, similarity matching, case variation, typo detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17361a49-9876-4c9e-ac89-ab034b6f8706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing column: 'customer'\n",
      "Total unique values: 74\n",
      "\n",
      "============================================================\n",
      "1. WHITESPACE ISSUES\n",
      "============================================================\n",
      "No whitespace issues found.\n",
      "\n",
      "============================================================\n",
      "2. SIMILAR VALUES (Potential Typos)\n",
      "============================================================\n",
      "  90.5% similar:\n",
      "    - 'Electricalsara Stores'\n",
      "    - 'Electricalsbea Stores'\n",
      "\n",
      "============================================================\n",
      "3. CASE VARIATIONS\n",
      "============================================================\n",
      "No case variations found.\n"
     ]
    }
   ],
   "source": [
    "#identify types of anomalies (typos, trailing, duplicates) in product column\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "def find_categorical_anomalies(df, column_name):\n",
    "    \"\"\"\n",
    "    Identify potential duplicates/typos in any categorical column.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame containing the data\n",
    "    column_name : str\n",
    "        Name of the categorical column to analyze\n",
    "    \"\"\"\n",
    "    values = df[column_name].dropna().unique()\n",
    "    \n",
    "    print(f\"Analyzing column: '{column_name}'\")\n",
    "    print(f\"Total unique values: {len(values)}\\n\")\n",
    "    \n",
    "    # 1. Check for leading/trailing whitespace\n",
    "    print(\"=\"*60)\n",
    "    print(\"1. WHITESPACE ISSUES\")\n",
    "    print(\"=\"*60)\n",
    "    whitespace_issues = []\n",
    "    for val in values:\n",
    "        if val != val.strip():\n",
    "            whitespace_issues.append(val)\n",
    "    \n",
    "    if whitespace_issues:\n",
    "        print(f\"Found {len(whitespace_issues)} values with whitespace:\")\n",
    "        for val in whitespace_issues:\n",
    "            print(f\"  '{val}' -> '{val.strip()}'\")\n",
    "    else:\n",
    "        print(\"No whitespace issues found.\")\n",
    "    \n",
    "    # 2. Check for very similar names (potential typos)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"2. SIMILAR VALUES (Potential Typos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    similar_pairs = []\n",
    "    values_list = list(values)\n",
    "    \n",
    "    for i in range(len(values_list)):\n",
    "        for j in range(i + 1, len(values_list)):\n",
    "            val1 = values_list[i].strip().lower()\n",
    "            val2 = values_list[j].strip().lower()\n",
    "            \n",
    "            # Calculate similarity ratio\n",
    "            similarity = SequenceMatcher(None, val1, val2).ratio()\n",
    "            \n",
    "            # If very similar (>0.8 threshold), flag it\n",
    "            if similarity > 0.9:\n",
    "                similar_pairs.append({\n",
    "                    'value1': values_list[i],\n",
    "                    'value2': values_list[j],\n",
    "                    'similarity': round(similarity, 3)\n",
    "                })\n",
    "    \n",
    "    if similar_pairs:\n",
    "        for pair in sorted(similar_pairs, key=lambda x: x['similarity'], reverse=True):\n",
    "            print(f\"  {pair['similarity']:.1%} similar:\")\n",
    "            print(f\"    - '{pair['value1']}'\")\n",
    "            print(f\"    - '{pair['value2']}'\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No highly similar values found.\")\n",
    "    \n",
    "    # 3. Check for case variations\n",
    "    print(\"=\"*60)\n",
    "    print(\"3. CASE VARIATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    case_variations = {}\n",
    "    for val in values:\n",
    "        normalized = val.strip().lower()\n",
    "        if normalized not in case_variations:\n",
    "            case_variations[normalized] = []\n",
    "        case_variations[normalized].append(val)\n",
    "    \n",
    "    duplicates = {k: v for k, v in case_variations.items() if len(v) > 1}\n",
    "    \n",
    "    if duplicates:\n",
    "        print(f\"Found {len(duplicates)} values with case variations:\")\n",
    "        for key, variants in duplicates.items():\n",
    "            print(f\"\\n  '{key}' has {len(variants)} variations:\")\n",
    "            for v in variants:\n",
    "                print(f\"    - '{v}'\")\n",
    "    else:\n",
    "        print(\"No case variations found.\")\n",
    "    \n",
    "    return {\n",
    "        'whitespace_issues': whitespace_issues,\n",
    "        'similar_pairs': similar_pairs,\n",
    "        'case_variations': duplicates\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "anomalies = find_categorical_anomalies(df_customer, 'customer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364e51c-2b8c-4278-93bc-656b62e9d7e0",
   "metadata": {},
   "source": [
    "### 5. Check what markets are available in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61314008-b690-49a0-9cc3-a92583d7b8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "market\n",
       "Australia          7\n",
       "Austria            8\n",
       "Bangladesh         5\n",
       "Brazil             2\n",
       "Canada            11\n",
       "Chile              2\n",
       "China              3\n",
       "Columbia           1\n",
       "France            10\n",
       "Germany           11\n",
       "India             18\n",
       "Indonesia          4\n",
       "Italy             11\n",
       "Japan             10\n",
       "Mexico             2\n",
       "Netherlands        9\n",
       "Newzealand         8\n",
       "Norway             9\n",
       "Pakistan           5\n",
       "Philiphines        6\n",
       "Poland             8\n",
       "Portugal          12\n",
       "South Korea        5\n",
       "Spain             11\n",
       "Sweden             5\n",
       "USA               15\n",
       "United Kingdom    11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customer['market'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc72469a-a972-46e0-bd66-0ebb1f755960",
   "metadata": {},
   "source": [
    "### 6. Check what platfrom are available in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a335fc5-d914-4c62-8c66-d4572ed1f1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "platform\n",
       "Brick & Mortar    150\n",
       "E-Commerce         59\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customer['platform'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93e98b-0c52-4df6-8e02-ac44fd46c636",
   "metadata": {},
   "source": [
    "### 7. Check what channels are available in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f764ee4c-7421-494a-99a2-e71dc8ea3f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "channel\n",
       "Direct          40\n",
       "Distributor      5\n",
       "Retailer       164\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customer['channel'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da80d8-18fc-4004-aef9-68847973adf4",
   "metadata": {},
   "source": [
    "### 8. Check grain and functional dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8204a805-567e-47ac-940d-31223a54cfdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_code</th>\n",
       "      <th>row_count</th>\n",
       "      <th>customer</th>\n",
       "      <th>market</th>\n",
       "      <th>platform</th>\n",
       "      <th>channel</th>\n",
       "      <th>customer_violates</th>\n",
       "      <th>market_violates</th>\n",
       "      <th>platform_violates</th>\n",
       "      <th>channel_violates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [customer_code, row_count, customer, market, platform, channel, customer_violates, market_violates, platform_violates, channel_violates]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Duplicate key check\n",
    "dupe_keys = (\n",
    "    df_customer\n",
    "        .groupby('customer_code')\n",
    "        .size()\n",
    "        .reset_index(name='row_count')\n",
    "        .query('row_count > 1')\n",
    ")\n",
    "\n",
    "# 2. Functional dependency check (customer_code → other columns)\n",
    "nunique_per_customer = (\n",
    "    df_customer\n",
    "        .groupby('customer_code')\n",
    "        .nunique()\n",
    ")\n",
    "\n",
    "fd_violations_mask = nunique_per_customer.gt(1).any(axis=1)\n",
    "fd_violations = nunique_per_customer[fd_violations_mask]\n",
    "\n",
    "# 3. Identify which columns violate FD for each customer\n",
    "violation_columns = (\n",
    "    nunique_per_customer\n",
    "        .gt(1)\n",
    "        .replace(False, pd.NA)\n",
    "        .dropna(how='all')\n",
    ")\n",
    "\n",
    "# 4. Combine everything into one report\n",
    "report = (\n",
    "    dupe_keys\n",
    "        .merge(fd_violations, on='customer_code', how='outer', suffixes=('_dupe', '_fd'))\n",
    "        .merge(violation_columns, on='customer_code', how='left', suffixes=('', '_violates'))\n",
    ")\n",
    "\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd91875-fc5d-488d-8338-ff30ac749412",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "No duplicates at grain level. All columns are functionaly dependent on customer_code. The grain is customer_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b3a1f-97d1-4ac2-82c0-5d97a3b3f00d",
   "metadata": {},
   "source": [
    "### 9. Checks whether a single customer appears in multiple markets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb4fe584-ef47-470a-8f19-0ec46474a951",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer\n",
       "Acclaimed Stores        2\n",
       "All-Out                 1\n",
       "Amazon                 25\n",
       "Argos (Sainsbury's)     3\n",
       "Atlas Stores            2\n",
       "                       ..\n",
       "Unity Stores            1\n",
       "Vijay Sales             1\n",
       "Viveks                  1\n",
       "Zone                    2\n",
       "walmart                 2\n",
       "Name: market, Length: 74, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customer.groupby('customer')['market'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f6363-c77f-4805-b3b9-cc108f921b01",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "Yes, a single customer appears in multiple markets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b2514-a9cb-4205-aa82-2ab1b898cd1d",
   "metadata": {},
   "source": [
    "### 10. Checks whether a single customer appears in multiple platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd2835ae-17ba-49c4-a7ac-82053225a2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer\n",
       "Acclaimed Stores       1\n",
       "All-Out                1\n",
       "Amazon                 1\n",
       "Argos (Sainsbury's)    1\n",
       "Atlas Stores           1\n",
       "                      ..\n",
       "Unity Stores           1\n",
       "Vijay Sales            1\n",
       "Viveks                 1\n",
       "Zone                   1\n",
       "walmart                1\n",
       "Name: platform, Length: 74, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customer.groupby('customer')['platform'].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de35bd-dc55-4088-a012-e836150907f9",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "one customer only exist in one platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb787c-73d4-4267-b39d-7af8b2c6cc0d",
   "metadata": {},
   "source": [
    "### 11. Check inconsistencies in customer_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "581c380a-e2ad-42c7-8ad6-af44d38fbc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing column: 'customer_code'\n",
      "Total unique values: 209\n",
      "\n",
      "============================================================\n",
      "1. WHITESPACE ISSUES\n",
      "============================================================\n",
      "No whitespace issues found.\n",
      "\n",
      "============================================================\n",
      "2. SIMILAR VALUES (Potential Typos)\n",
      "============================================================\n",
      "No highly similar values found.\n",
      "============================================================\n",
      "3. CASE VARIATIONS\n",
      "============================================================\n",
      "No case variations found.\n"
     ]
    }
   ],
   "source": [
    "cc_code_anomalies = find_categorical_anomalies(df_customer, 'customer_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c205420a-3e79-4d96-8283-65ebc03e6f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
